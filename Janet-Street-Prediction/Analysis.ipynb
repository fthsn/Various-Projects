{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"####################################################################\n## Part 0: Init\n####################################################################\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-23T19:08:54.111054Z","iopub.execute_input":"2021-05-23T19:08:54.11147Z","iopub.status.idle":"2021-05-23T19:08:54.12326Z","shell.execute_reply.started":"2021-05-23T19:08:54.111432Z","shell.execute_reply":"2021-05-23T19:08:54.122296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"####################################################################\n## Part 1: DataPrep 1: Creating Train and Test Sets\n####################################################################\n\nraw_train = pd.read_csv('/kaggle/input/jane-street-market-prediction/train.csv',nrows=300000)\n#raw_train = raw_train.drop(['Unnamed: 0'],axis=1)\ntrain = raw_train.copy()\ntrain = train[train['weight'] != 0]\ny = train.loc[:,'resp']\ny = np.where(y>0,1,0)\n#X = train.iloc[:,7:(len(train.columns)-1)]\nX = train\nfrom sklearn.model_selection import train_test_split\nAll_TrainValid, All_Test, y_TrainValid, y_Test = train_test_split(X, y, test_size=0.2, random_state=42,shuffle=False)\nprint('All_TrainValid: {} All_Test: {} y_TrainValid: {} y_Test: {} '.format(All_TrainValid.shape, All_Test.shape, y_TrainValid.shape, y_Test.shape))\n\n# All_TrainValid['date'].value_counts()\n# All_Test['date'].value_counts()\n\nfeatures = [c for c in All_TrainValid.columns if 'feature' in c]\nX_TrainValid = All_TrainValid[features]\nX_Test = All_Test[features]","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:09:01.005325Z","iopub.execute_input":"2021-05-23T19:09:01.005689Z","iopub.status.idle":"2021-05-23T19:09:02.822409Z","shell.execute_reply.started":"2021-05-23T19:09:01.005659Z","shell.execute_reply":"2021-05-23T19:09:02.821518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"####################################################################\n## Part 2: DataPrep 2: Missing Inputation, Input Scaling\n####################################################################\n\n# Missing Inputation\nfrom sklearn.impute import SimpleImputer\n\nimputerF = SimpleImputer(missing_values=np.nan,strategy='mean')\nfloat_TrainValid = X_TrainValid.select_dtypes(include='float')\nX_TrainValid.loc[:,float_TrainValid.columns] = imputerF.fit_transform(float_TrainValid)\n#Input Test Set\nfloat_Test = X_Test.select_dtypes(include='float')\nX_Test.loc[:,float_Test.columns] = imputerF.transform(float_Test)\n\n\nimputerI = SimpleImputer(missing_values=np.nan,strategy='median')\nint_TrainValid = X_TrainValid.select_dtypes(include='int')\nX_TrainValid.loc[:,int_TrainValid.columns] = imputerI.fit_transform(int_TrainValid)\n#Input Test Set\nint_Test = X_Test.select_dtypes(include='int')\nX_Test.loc[:,int_Test.columns] = imputerI.transform(int_Test)\n\n# Scaling Inputs\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_TrainValid = sc.fit_transform(X_TrainValid)\n#Scale Test Set\nX_Test = sc.transform(X_Test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"####################################################################\n## Part 3: Model Training\n####################################################################\n\nimport tensorflow as tf\n\nimport random\nimport os\ndef set_all_seeds(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n\nset_all_seeds(42)\n\nmodel = tf.keras.models.Sequential(\n    [\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(64,activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(64,activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(1,activation='sigmoid'),\n    ]\n)\noptimizer = 'adam'\nmodel.compile(optimizer, loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.001),metrics=tf.keras.metrics.Precision(name='precision'))\nmodel.fit(X_TrainValid,y_TrainValid,epochs=40, batch_size=int(len(X_TrainValid)/5))\n\ndef utility_score_bincount(date, weight, resp, action):\n    count_i = len(np.unique(date))\n    Pi = np.bincount(date, weight * resp * action)\n    t = np.sum(Pi) / np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 / count_i)\n    u = np.clip(t, 0, 6) * np.sum(Pi)\n    return u\n\ny_pred = model.predict(X_TrainValid).reshape((-1,))\nprint(utility_score_bincount(All_TrainValid['date'].values, All_TrainValid['weight'].values,\n                       All_TrainValid['resp'].values, np.where(y_pred>0.5,1,0))/len(X_TrainValid))\n\ny_pred = model.predict(X_Test).reshape((-1,))\nprint(utility_score_bincount(All_Test['date'].values, All_Test['weight'].values,\n                       All_Test['resp'].values, np.where(y_pred>0.5,1,0))/len(X_Test))\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:09:11.576272Z","iopub.execute_input":"2021-05-23T19:09:11.576631Z","iopub.status.idle":"2021-05-23T19:09:17.970741Z","shell.execute_reply.started":"2021-05-23T19:09:11.576598Z","shell.execute_reply":"2021-05-23T19:09:17.969885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"####################################################################\n## Part 4: Model Use for Testing\n####################################################################\n\nimport janestreet\nenv = janestreet.make_env()\n\nf_mean = np.mean(train[features[1:]].values,axis=0)\nfrom tqdm import tqdm\nfor (test_df, pred_df) in tqdm(env.iter_test()): \n    if test_df['weight'].item() > 0:\n        \n        test_df = test_df.loc[:, features]\n        #Input Test Set: float\n        float_Test = test_df.loc[:,float_TrainValid.columns]\n        test_df.loc[:,float_Test.columns] = imputerF.transform(float_Test)\n        #Input Test Set: int\n        int_Test = test_df.loc[:,int_TrainValid.columns]\n        test_df.loc[:,int_Test.columns] = imputerI.transform(int_Test)\n        #Scale Test Set\n        test_df = sc.transform(test_df)\n\n        x_tt = test_df\n        x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n            \n        pred = np.median(model(x_tt))\n        pred_df.action = np.where(pred >= 0.5, 1, 0).astype(int)\n        \n    else:\n        pred_df.action = 0\n        \n    env.predict(pred_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}